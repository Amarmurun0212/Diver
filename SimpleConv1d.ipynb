{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnYvWyG9hcyVrL60GLx+07",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amarmurun0212/Diver/blob/main/SimpleConv1d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PR69mkxJ-VDA"
      },
      "outputs": [],
      "source": [
        "from platform import release\n",
        "from re import L\n",
        "import numpy as np\n",
        "import math\n",
        "from keras.datasets import mnist\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid:\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return self.sigmoid(A)\n",
        "    def backward(self, dZ):\n",
        "        _sig = self.sigmoid(self.A)\n",
        "        return dZ * (1 - _sig)*_sig\n",
        "    def sigmoid(self, X):\n",
        "        return 1 / (1 + np.exp(-X))\n",
        "\n",
        "class Tanh:\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.tanh(A)\n",
        "    def backward(self, dZ):\n",
        "        return dZ * (1 - (np.tanh(self.A))**2)\n",
        "\n",
        "class Softmax:\n",
        "    def forward(self, X):\n",
        "        self.Z = np.exp(X) / np.sum(np.exp(X), axis=1).reshape(-1,1)\n",
        "        return self.Z\n",
        "    def backward(self, Y):\n",
        "        self.loss = self.loss_func(Y)\n",
        "        return self.Z - Y\n",
        "    def loss_func(self, Y, Z=None):\n",
        "        if Z is None:\n",
        "            Z = self.Z\n",
        "        return (-1)*np.average(np.sum(Y*np.log(Z), axis=1))\n",
        "\n",
        "class ReLU:\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.clip(A, 0, None)\n",
        "    def backward(self, dZ):\n",
        "        return dZ * np.clip(np.sign(self.A), 0, None)\n",
        "\n",
        "class FC:\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer,activation):\n",
        "        self.optimizer = optimizer\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.B = initializer.B(n_nodes2)\n",
        "        self.activation = activation\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        A = X@self.W + self.B\n",
        "        return self.activation.forward(A)\n",
        "    def backward(self, dA):\n",
        "        dA = self.activation.backward(dA)\n",
        "        dZ = dA@self.W.T\n",
        "        self.dB = np.sum(dA, axis=0)\n",
        "        self.dW = self.X.T@dA\n",
        "        self.optimizer.update(self)\n",
        "        return dZ\n",
        "\n",
        "class XavierInitializer:\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        self.sigma = math.sqrt(1 / n_nodes1)\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "    def B(self, n_nodes2):\n",
        "        B = self.sigma * np.random.randn(n_nodes2)\n",
        "        return B\n",
        "    \n",
        "class HeInitializer():\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        self.sigma = math.sqrt(2 / n_nodes1)\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "    def B(self, n_nodes2):\n",
        "        B = self.sigma * np.random.randn(n_nodes2)\n",
        "        return B\n",
        "        \n",
        "class SimpleInitializer:\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "    def W(self, *shape):\n",
        "        W = self.sigma * np.random.randn(*shape)\n",
        "        return W\n",
        "    def B(self, *shape):\n",
        "        B = self.sigma * np.random.randn(*shape)\n",
        "        return B\n",
        "\n",
        "class SimpleInitializerConv1d:\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "    def W(self, *shape):\n",
        "        W = self.sigma * np.random.randn(*shape)\n",
        "        return W\n",
        "    def B(self, *shape):\n",
        "        B = self.sigma * np.random.randn(*shape)\n",
        "        return B\n",
        "\n",
        "class SGD:\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "    def update(self, layer):\n",
        "        layer.W -= self.lr * layer.dW\n",
        "        layer.B -= self.lr * layer.dB\n",
        "        return\n",
        "\n",
        "class AdaGrad:\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "        self.HW = 1\n",
        "        self.HB = 1\n",
        "    def update(self, layer):\n",
        "        self.HW += layer.dW**2\n",
        "        self.HB += layer.dB**2\n",
        "        layer.W -= self.lr * np.sqrt(1/self.HW) * layer.dW\n",
        "        layer.B -= self.lr * np.sqrt(1/self.HB) * layer.dB\n",
        "class GetMiniBatch:\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int64)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1] \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]"
      ],
      "metadata": {
        "id": "4VcnbLiMAUTa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Problem 1] Creating a one-dimensional convolutional layer class with a limited number of channels"
      ],
      "metadata": {
        "id": "JKTFeznHAevw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleConv1d():\n",
        "    '''\n",
        "    1d conv layer\n",
        "    Parameters\n",
        "    -----------------\n",
        "    n_nodes1 : int\n",
        "        Number of nodes in the previous layer\n",
        "    n_nodes 2 : int\n",
        "        Number of nodes in later layers\n",
        "    Initializer : Instances of initialization method\n",
        "    Optimizer : Instances of optimization method\n",
        "    '''\n",
        "    def __init__(self, output_channel, input_channel, filter_size, padding = 0, stride = 1, initializer = None, optimizer = None, activation = None):\n",
        "        self.initializer = initializer\n",
        "        self.optimizer = optimizer \n",
        "        self.activation = activation\n",
        "\n",
        "        self.W = self.initializer.W(output_channel, input_channel, filter_size)\n",
        "        self.B = self.initializer.B(output_channel)\n",
        "\n",
        "    def output_size_calculation(self, n_in, filter_size, padding=0, stride=1):\n",
        "        \"\"\"\n",
        "        Calculate output size after 1d convolution\n",
        "\n",
        "        Parameters\n",
        "        -----------------\n",
        "        n_in: Input size   \n",
        "        F: filter size \n",
        "        P: padding number \n",
        "        S: stride number \n",
        "\n",
        "        Return\n",
        "        -----------------\n",
        "        n_out: size of output\n",
        "        \"\"\"\n",
        "        n_out = int((n_in + 2*padding - filter_size) / stride + 1)   \n",
        "        return n_out\n",
        "\n",
        "    def forward(self, X):\n",
        "        '''\n",
        "        Calculate forward propagation\n",
        "        Parameters\n",
        "        --------------\n",
        "        x : ndarray shape with (batch_size, n_nodes1)\n",
        "            training feature\n",
        "\n",
        "        returns\n",
        "        ---------------\n",
        "        A : ndarray shape with (batch_size, n_nodes2)\n",
        "        '''\n",
        "        self.X = X\n",
        "        N,INC, Feature = X.shape\n",
        "        OCH, INC, FS = self.W.shape\n",
        "        OUT = self.output_size_calculation(Feature, FS, 0,1)\n",
        "        self.size = N,INC,OCH,FS,OUT\n",
        "        A = np.zeros([N,OCH,OUT])\n",
        "\n",
        "        for n in range(N):\n",
        "            for och in range(OCH):\n",
        "                for ich in range(INC):\n",
        "                    for m in range(OUT):\n",
        "                        A[n,och,m] += np.sum(X[n, ich,m:m+FS]*self.W[och,ich,:]) \n",
        "        \n",
        "        A += self.B[:,None]\n",
        "        A = self.activation.forward(A)\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        '''\n",
        "        Calculate backward propagation\n",
        "        Parameters\n",
        "        --------------\n",
        "        x: ndarray\n",
        "            training feature\n",
        "        w: ndarray\n",
        "            weight\n",
        "        da: ndarray\n",
        "            backpropagation value\n",
        "        '''\n",
        "        dA = self.activation.backward(dZ)\n",
        "        self.dB = np.mean(np.sum(dA, axis=2), axis = 0)\n",
        "\n",
        "        self.dW = np.zeros(self.W.shape)\n",
        "        dZ = np.zeros(self.X.shape)\n",
        "\n",
        "        N,INC,OCH,FS,OUT = self.size\n",
        "        for n in range(N):\n",
        "            for och in range(OCH):\n",
        "                for ich in range(INC):\n",
        "                    for fs in range(FS):\n",
        "                        for m in range(OUT):\n",
        "                            self.dW[och,ich,fs] += self.X[n,ich,fs+m]*dA[n,och,m]\n",
        "                            dZ[n,ich,fs+m] += self.W[och,ich,fs]*dA[n,och,m]\n",
        "        \n",
        "        self = self.optimizer.update(self)\n",
        "\n",
        "        return dZ\n",
        "\n",
        "class Scratch1dCNNClassifier:\n",
        "    \"\"\"\n",
        "    1d conv layer \n",
        "    \"\"\"\n",
        "    def __init__(self, NN, CNN, n_epoch = 10, n_batch = 5, verbose = False):\n",
        "        \n",
        "        self.n_batch = n_batch\n",
        "        self.n_epoch = n_epoch\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self.log_loss = np.zeros(self.n_epoch)\n",
        "        self.log_acc = np.zeros(self.n_epoch)\n",
        "        self.NN = NN\n",
        "        self.CNN = CNN\n",
        "    def loss_function(self, y, yt):\n",
        "        delta = 1e-7\n",
        "        temp = -np.mean(yt*np.log(y + delta))\n",
        "        return temp\n",
        "    \n",
        "    def accuracy(self, y, yt):\n",
        "        return accuracy_score(y,yt)\n",
        "    \n",
        "    def fit(self, X, y, X_val = False, y_val = False):\n",
        "        \"\"\"\n",
        "        Train a cnn classifier\n",
        "\n",
        "        Parameters\n",
        "        ---------------\n",
        "        X : ndarray shape with (n_samples, n_features)\n",
        "            features of training data\n",
        "        y : ndarray shape with (n_samples, )\n",
        "            True label of training data\n",
        "        X_val : ndarray shape with (n_samples, n_features)\n",
        "            features of validation data\n",
        "        y_val : ndarray shape with (n_samples, )\n",
        "            True label of validation data\n",
        "        \"\"\"\n",
        "\n",
        "        for epoch in range(self.n_epoch):\n",
        "            self.loss = 0\n",
        "            get_mini_batch = GetMiniBatch(X,y, batch_size=self.n_batch)\n",
        "            for mini_X_train, mini_y_train in get_mini_batch:\n",
        "                forward_data = mini_X_train.reshape(self.n_batch,1,-1)\n",
        "                for layer in range(len(self.CNN)):\n",
        "                    forward_data = self.CNN[layer].forward(forward_data)\n",
        "\n",
        "                record_shape = forward_data.shape\n",
        "                forward_data = forward_data.reshape(self.n_batch, -1)\n",
        "\n",
        "                for layer in range(len(self.NN)):\n",
        "                    forward_data = self.NN[layer].forward(forward_data)\n",
        "                \n",
        "                Z = forward_data\n",
        "\n",
        "                backward_data = (Z - mini_y_train)/self.n_batch\n",
        "                for layer in range(len(self.NN)-1,-1,-1):\n",
        "                    backward_data = self.NN[layer].backward(backward_data)\n",
        "\n",
        "                backward_data = backward_data.reshape(record_shape)\n",
        "\n",
        "                for layer in range(len(self.CNN)-1,-1,-1):\n",
        "                    backward_data = self.CNN[layer].backward(backward_data)\n",
        "                \n",
        "                self.loss += self.loss_function(Z, mini_y_train)\n",
        "\n",
        "            self.log_loss[epoch] = self.loss/len(get_mini_batch)\n",
        "            self.log_acc[epoch] = self.accuracy(self.predict(X), np.argmax(y, axis = 1))\n",
        "\n",
        "            if self.verbose:\n",
        "                print('epoch:{} loss:{} acc:{}'.format(epoch, self.loss/self.n_batch, self.log_acc[epoch]))   \n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using a neural network classifier\n",
        "        \n",
        "        Parameters\n",
        "        ---------------\n",
        "        X : ndarray shape with (n_samples, n_features)\n",
        "            sample of dataset\n",
        "\n",
        "        Returns\n",
        "        ---------------\n",
        "        pred : ndarray (n_samples, 1)\n",
        "        \"\"\"\n",
        "        pred_data = X[:, np.newaxis, :]\n",
        "\n",
        "        for layer in range(len(self.CNN)):\n",
        "            pred_data = self.CNN[layer].forward(pred_data)\n",
        "        \n",
        "        pred_data = pred_data.reshape(len(X),-1)\n",
        "        for layer in range(len(self.CNN)):\n",
        "            pred_data = self.NN[layer].forward(pred_data)\n",
        "        \n",
        "        pred = np.argmax(pred_data, axis = 1)\n",
        "        return pred"
      ],
      "metadata": {
        "id": "2MCiDxk9AwNL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Problem 2] Calculation of output size after one-dimensional convolution"
      ],
      "metadata": {
        "id": "8dlpMRWBBBv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def output_size_calculation( n_in, filter_size, padding=0, stride=1):\n",
        "        \"\"\"\n",
        "        Calculate output size after 1d convolution\n",
        "\n",
        "        Parameters\n",
        "        -----------------\n",
        "        n_in: Input size   \n",
        "        F: filter size \n",
        "        P: padding number \n",
        "        S: stride number \n",
        "\n",
        "        Return\n",
        "        -----------------\n",
        "        n_out: size of output\n",
        "        \"\"\"\n",
        "        n_out = int((n_in + 2*padding - filter_size) / stride + 1)   \n",
        "        return n_out\n",
        "\n",
        "a = output_size_calculation(4,3,0,1)\n",
        "print(\"output:\", a)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B27KD5HFBCoR",
        "outputId": "d6064363-0849-48d6-f40d-11fed3a3d7c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Problem 3] One-dimensional convolutional layer experiment with small arrays\n",
        "\n",
        "Check if the forward and backpropagation is working correctly for the small array shown below."
      ],
      "metadata": {
        "id": "drsb0HAuBK7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([1,2,3,4])\n",
        "w = np.array([3,5,7])\n",
        "b = np.array([1])\n",
        "\n",
        "# forward propagation\n",
        "a = np.zeros(a)\n",
        "for i in range(len(a)):\n",
        "    x_temp = X[i:i+len(w)]\n",
        "    a[i] = np.sum(x_temp*w)+b\n",
        "print(a)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hzfj964BLmp",
        "outputId": "a9355b77-fa16-4515-ba4e-9920e51fc762"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[35. 50.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "back propagation "
      ],
      "metadata": {
        "id": "Sqwy7L0eCsww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delta_a = np.array([10,20])\n",
        "delta_b = np.sum(delta_a)\n",
        "print(delta_b)\n",
        "\n",
        "delta_w = np.zeros([len(w)])\n",
        "for i in range(len(w)):\n",
        "    x_temp = X[i:i+len(delta_a)]\n",
        "    delta_w[i] = np.sum(x_temp*delta_a)\n",
        "print(delta_w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfDvR8moCth4",
        "outputId": "2e9d5fb8-847a-4d7b-b89c-8c27337d1c5d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n",
            "[ 50.  80. 110.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "delta_x = np.zeros(len(X))\n",
        "\n",
        "for i in range(len(X)):\n",
        "    zero = np.zeros(len(delta_a)-1)\n",
        "    w_padded = np.concatenate([zero,w,zero], axis =0)\n",
        "    w_temp = w_padded[i:i+len(delta_a)]\n",
        "    delta_x[i] = np.sum(w_temp*delta_a[::-1])\n",
        "print(delta_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9e-MsIkCzIo",
        "outputId": "91339ef0-960d-4ccd-da85-c5d0536f72bd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 30. 110. 170. 140.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[1,2,3,4],[2,3,4,5]])\n",
        "w = np.array([[[1,1,2],[2,1,1]], [[2,1,1],[1,1,1]], [[1,1,1],[1,1,1]]])\n",
        "b = np.array([1,2,3])\n",
        "\n",
        "a = np.zeros([3, output_size_calculation(4,3,0,1)])\n",
        "\n",
        "for och in range(w.shape[0]):\n",
        "    for ch in range(w.shape[1]):\n",
        "        for m in range(a.shape[1]):\n",
        "            a[och,m] += np.sum(x[ch, m:m+w.shape[2]]* w[och,ch,:])\n",
        "\n",
        "a += b[:,None]\n",
        "print(\"print forward prop:\", a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vjj7AZEfC2MI",
        "outputId": "d90118b6-390a-41a0-e77d-6a13313d1d17"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "print forward prop: [[21. 29.]\n",
            " [18. 25.]\n",
            " [18. 24.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "delta_a = np.array([[9,11], [32,35],[52,56]])\n",
        "\n",
        "print(\"delta_a:\\n\", delta_a)\n",
        "print(\"delta_a.shape:\\n\", delta_a.shape)\n",
        "\n",
        "delta_b = np.sum(delta_a, axis = 1)\n",
        "print(\"delta_b: \", delta_b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ztguQSYC5no",
        "outputId": "6e9dafad-8e89-46e8-e2f3-48aee55b19fb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "delta_a:\n",
            " [[ 9 11]\n",
            " [32 35]\n",
            " [52 56]]\n",
            "delta_a.shape:\n",
            " (3, 2)\n",
            "delta_b:  [ 20  67 108]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "delta_w = np.zeros([3,2,3])\n",
        "\n",
        "for och in range(w.shape[0]):\n",
        "    for ich in range(w.shape[1]):\n",
        "        for fs in range(w.shape[2]):\n",
        "            for m in range(2):\n",
        "                delta_w[och,ich,fs] += (x[ich, fs+m]*delta_a[och,m])\n",
        "\n",
        "print(\"delta_w:\\n\", delta_w)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9fuEYMwC62I",
        "outputId": "f7d50fdd-411d-479c-8c57-ff2915580848"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "delta_w:\n",
            " [[[ 31.  51.  71.]\n",
            "  [ 51.  71.  91.]]\n",
            "\n",
            " [[102. 169. 236.]\n",
            "  [169. 236. 303.]]\n",
            "\n",
            " [[164. 272. 380.]\n",
            "  [272. 380. 488.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "delta_x = np.zeros([2,4])\n",
        "\n",
        "for och in range(w.shape[0]):\n",
        "    for ich in range(w.shape[1]):\n",
        "        for fs in range(w.shape[2]):\n",
        "            for m in range(2):\n",
        "                delta_x[ich,fs+m] += w[och, ich,fs] * delta_a[och,m]\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(\"X_train data shape: \", X_train.shape) # (60000, 28, 28)\n",
        "print(\"X_test data shape: \", X_test.shape) # (10000, 28, 28)\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsMDp1wRDAzR",
        "outputId": "da750ce1-8cbd-454f-eefa-6a0e98f94b00"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train data shape:  (60000, 28, 28)\n",
            "X_test data shape:  (10000, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "de6nBpCpDJTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBWTgkd7DL8n",
        "outputId": "ccd16d76-8c67-4d19-c7e3-9d1cafa145f4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-a7190662b343>:1: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  X_train = X_train.astype(np.float)\n",
            "<ipython-input-13-a7190662b343>:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  X_test = X_test.astype(np.float)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the correct label is an integer from 0 to 9, but it is converted to a one-hot representation"
      ],
      "metadata": {
        "id": "vpZ3vcHmDQox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train.reshape(-1,1))\n",
        "y_val_one_hot = enc.fit_transform(y_val.reshape(-1,1))\n",
        "print(\"Train dataset:\", X_train.shape) # (48000, 784)\n",
        "print(\"Validation dataset:\", X_val.shape) # (12000, 784)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCdrmak_DRO4",
        "outputId": "009cecd3-f853-4c45-b997-18e07218c710"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset: (48000, 784)\n",
            "Validation dataset: (12000, 784)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NN = {0: FC(15640, 400, HeInitializer(), AdaGrad(0.01), Tanh()),\n",
        "    1: FC(400, 200, HeInitializer(), AdaGrad(0.01), Tanh()),\n",
        "    2: FC(200, 10, SimpleInitializer(0.01), AdaGrad(0.01), Softmax()),}\n",
        "\n",
        "CNN = {0: SimpleConv1d(output_channel=20, input_channel=1, filter_size=3, padding=0, stride=1, \n",
        "            initializer=SimpleInitializerConv1d(0.01), optimizer=SGD(0.01), activation=Sigmoid()),}\n",
        "\n",
        "cnn1d = Scratch1dCNNClassifier(NN=NN, CNN=CNN, n_epoch=10, n_batch=100, verbose = True)\n",
        "cnn1d.fit(X_train[0:1000], y_train_one_hot[0:1000])\n",
        "\n",
        "y_pred = cnn1d.predict(X_val[0:500])\n",
        "acc = accuracy_score(y_val[0:500], y_pred)\n",
        "print(\"Accuracy:\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6_vm4aeDXSf",
        "outputId": "bf85539b-9764-4a2f-8e35-901a90809c65"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0 loss:0.023027905497615433 acc:0.0\n",
            "epoch:1 loss:0.02301688050563829 acc:0.0\n",
            "epoch:2 loss:0.02301696488264013 acc:0.0\n",
            "epoch:3 loss:0.0230170112651498 acc:0.0\n",
            "epoch:4 loss:0.0230170496444093 acc:0.0\n",
            "epoch:5 loss:0.023017082497332662 acc:0.0\n",
            "epoch:6 loss:0.02301711086068643 acc:0.0\n",
            "epoch:7 loss:0.023017135551555296 acc:0.0\n",
            "epoch:8 loss:0.0230171572342788 acc:0.0\n",
            "epoch:9 loss:0.02301717643870691 acc:0.0\n",
            "Accuracy: 0.0\n"
          ]
        }
      ]
    }
  ]
}