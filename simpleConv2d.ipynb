{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7TSvVoB3M5mPgtof/yYOm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amarmurun0212/Diver/blob/main/simpleConv2d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uslAJdiKc2n"
      },
      "outputs": [],
      "source": [
        "from turtle import forward\n",
        "import numpy as np\n",
        "import math\n",
        "from keras.datasets import mnist\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "User defined classes"
      ],
      "metadata": {
        "id": "Gp0DxyKGLQKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FC():\n",
        "    \"\"\"\n",
        "    FC layers from number of nodes n_nodes1 to n_nodes2\n",
        "    Parameters\n",
        "    --------------\n",
        "    n_nodes1 : int\n",
        "        Number of nodes in the previous layer\n",
        "    n_nodes2 : int\n",
        "        Number of nodes in later layer\n",
        "    initializer : Instances of initialization methods\n",
        "    optimizer : Instances of optimization methods\n",
        "    activation : Activation function\n",
        "\n",
        "    Returns\n",
        "    --------------\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer, activation):\n",
        "        self.optimizer = optimizer\n",
        "        self.activation = activation\n",
        "\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.B = initializer.B(n_nodes2)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward\n",
        "        Parameters\n",
        "        ----------------\n",
        "        X : ndarray shape with (batch_size, n_nodes1)\n",
        "            Input\n",
        "        Returns\n",
        "        ----------------\n",
        "        A : ndarray shape with (batch_size, n_nodes2)\n",
        "            Output\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        A = X @ self.W + self.B\n",
        "        return self.activation.forward(A)\n",
        "    \n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Backward\n",
        "        Parameters\n",
        "        ----------------\n",
        "        dA : ndarray shape with (batch_size, n_nodes2)\n",
        "            The gradient flowed in from behind\n",
        "        Returns\n",
        "        ----------------\n",
        "        dZ : ndarray shape with (batch_size, n_nodes1)\n",
        "            forward slope\n",
        "        \"\"\"\n",
        "        dA = self.activation.backward(dA)\n",
        "        dZ = dA@self.W.T\n",
        "        self.dB = np.sum(dA, axis=0)\n",
        "        self.dW = self.X.T@dA\n",
        "        self.optimizer.update(self)\n",
        "        return dZ\n",
        "\n",
        "class SimpleInitializerConv2d():\n",
        "    \"\"\"\n",
        "    Initialization with Gaussian distribution\n",
        "    Parameters\n",
        "    ----------------\n",
        "    sigma: float\n",
        "        standard deviation of Gaussian distribution\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma = 0.01):\n",
        "        self.sigma = sigma\n",
        "    def W(self, *shape):\n",
        "        \"\"\"\n",
        "        Initializing weights\n",
        "        Parameters:\n",
        "        F,C,FH,FW\n",
        "        Returns\n",
        "        --------------------\n",
        "        W: weights\n",
        "        \"\"\"\n",
        "        W = self.sigma * np.random.randn(*shape)\n",
        "        return W\n",
        "    def B(self, *shape):\n",
        "        \"\"\"\n",
        "        Initializing bias\n",
        "        Parameters:\n",
        "        F\n",
        "        Returns\n",
        "        --------------------\n",
        "        B: biases\n",
        "        \"\"\"\n",
        "        B = self.sigma * np.random.randn(*shape)\n",
        "        return B\n",
        "\n",
        "class SimpleInitializer():\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "    def W(self, *shape):\n",
        "        W = self.sigma * np.random.randn(*shape)\n",
        "        return W\n",
        "    def B(self, *shape):\n",
        "        B = self.sigma * np.random.randn(*shape)\n",
        "        return B\n",
        "\n",
        "class XavierInitializer():\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        self.sigma = math.sqrt(1 / n_nodes1)\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "    def B(self, n_nodes2):\n",
        "        B = self.sigma * np.random.randn(n_nodes2)\n",
        "        return B\n",
        "    \n",
        "class HeInitializer():\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        self.sigma = math.sqrt(2 / n_nodes1)\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "    def B(self, n_nodes2):\n",
        "        B = self.sigma * np.random.randn(n_nodes2)\n",
        "        return B\n",
        "\n",
        "class SGD():\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "    def update(self, layer):\n",
        "        layer.W -= self.lr * layer.dW\n",
        "        layer.B -= self.lr * layer.dB\n",
        "        return layer\n",
        "\n",
        "class AdaGrad():\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "        self.HW = 1\n",
        "        self.HB = 1\n",
        "    def update(self, layer):\n",
        "        self.HW += layer.dW**2\n",
        "        self.HB += layer.dB**2\n",
        "        layer.W -= self.lr * np.sqrt(1/self.HW) * layer.dW\n",
        "        layer.B -= self.lr * np.sqrt(1/self.HB) * layer.dB\n",
        "\n",
        "\n",
        "class Sigmoid():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return self.sigmoid(A)\n",
        "    def backward(self, dZ):\n",
        "        _sig = self.sigmoid(self.A)\n",
        "        return dZ * (1 - _sig)*_sig\n",
        "    def sigmoid(self, X):\n",
        "        return 1 / (1 + np.exp(-X))\n",
        "\n",
        "class Tanh():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.tanh(A)\n",
        "    def backward(self, dZ):\n",
        "        return dZ * (1 - (np.tanh(self.A))**2)\n",
        "\n",
        "class Softmax():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def forward(self, X):\n",
        "        self.Z = np.exp(X) / np.sum(np.exp(X), axis=1).reshape(-1,1)\n",
        "        return self.Z\n",
        "    def backward(self, Y):\n",
        "        self.loss = self.loss_func(Y)\n",
        "        return self.Z - Y\n",
        "    def loss_func(self, Y, Z=None):\n",
        "        if Z is None:\n",
        "            Z = self.Z\n",
        "        return (-1)*np.average(np.sum(Y*np.log(Z), axis=1))\n",
        "\n",
        "class ReLU():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.maximum(self.A, 0)\n",
        "    def backward(self, dZ):\n",
        "        return np.where(self.A>0,dZ,0)\n",
        "\n",
        "class GetMiniBatch:\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1] \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]"
      ],
      "metadata": {
        "id": "QgtOOOrXLSUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Problem 1] Creating a 2D convolutional layer"
      ],
      "metadata": {
        "id": "S8tIBg2HLjUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleConv2d():\n",
        "    \"\"\"\n",
        "    Implementation of simple 2d convolution\n",
        "    Parameters\n",
        "    -------------\n",
        "    Initializer : Instances of initialization methods\n",
        "    Optimizer : Instances of optimization methods \n",
        "    Returns \n",
        "    -------------\n",
        "    \"\"\"\n",
        "    def __init__(self, F, C, FH, FW, P, S,initializer=None,optimizer=None,activation=None):\n",
        "        self.P = P\n",
        "        self.S = S\n",
        "        self.initializer = initializer\n",
        "        self.optimizer = optimizer\n",
        "        self.activation = activation\n",
        "\n",
        "        self.W = self.initializer.W(F,C,FH,FW)\n",
        "        self.B = self.initializer.B(F)\n",
        "    \n",
        "    def output_shape2d(self,H,W,PH,PW,FH,FW,SH,SW):\n",
        "        OH = (H + 2 * PH - FH)/SH + 1\n",
        "        OW = (W + 2 * PW - FW)/SW + 1\n",
        "        return int(OH),int(OW)\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        forward\n",
        "        Parameters\n",
        "        -------------------\n",
        "        X : ndarray shape with (batch_size, n_nodes1)\n",
        "            Input\n",
        "\n",
        "        Returns\n",
        "        A : ndarray shape with (batch_size, n_nodes2)\n",
        "            Output\n",
        "        -------------------\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        N,C,H,W  = self.X.shape\n",
        "        F,C,FH,FW = self.W.shape\n",
        "\n",
        "        OH, OW = self.output_shape2d(H,W,self.P, self.P, FH, FW, self.S, self.S)\n",
        "        self.params = N,C,H,W,F,FH,FW,OH,OW\n",
        "\n",
        "        A = np.zeros([N,F,OH,OW])\n",
        "\n",
        "        self.X_pad = np.pad(self.X,((0,0),(0,0),(self.P, self.P), (self.P,self.P)))\n",
        "\n",
        "\n",
        "        for n in range(N):\n",
        "            for ch in range(F):\n",
        "                for row in range(0,H,self.S):\n",
        "                    for col in range(0,W,self.S):\n",
        "                        A[n,ch,row,col] = np.sum(self.X_pad[n,:,row:row+FH, col:col+FW]*self.W[ch,:,:,:])+self.B[ch]\n",
        "        \n",
        "        return self.activation.forward(A)\n",
        "       \n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Backward\n",
        "        Parameters\n",
        "        ----------------\n",
        "        dA : ndarray shape with (batch_size, n_nodes2)\n",
        "            The gradient flowed in from behind\n",
        "        Returns\n",
        "        ----------------\n",
        "        dZ : ndarray shape with (batch_size, n_nodes1)\n",
        "            forward slope\n",
        "        \"\"\"\n",
        "\n",
        "        dA = self.activation.backward(dA)\n",
        "        N,C,H,W,F,FH,FW,OH,OW = self.params\n",
        "\n",
        "        dZ = np.zeros(self.X_pad.shape)\n",
        "        self.dW = np.zeros(self.W.shape)\n",
        "        self.dB = np.zeros(self.B.shape)\n",
        "\n",
        "        # dZ\n",
        "        # Batch\n",
        "        for n in range(N):\n",
        "            for ch in range(F):\n",
        "                for row in range(0,H,self.S):\n",
        "                    for col in range(0,W, self.S):\n",
        "                        dZ[n,:,row:row+FH, col:col+FW] += dA[n,ch,row,col]*self.W[ch,:,:,:]\n",
        "        \n",
        "        d1_rows = range(self.P), range(H+self.P, H+2*self.P,1)\n",
        "        d1_cols = range(self.P), range(W+self.P, W+2*self.P,1)\n",
        "\n",
        "        dZ = np.delete(dZ, d1_rows, axis =2)\n",
        "        dZ = np.delete(dZ,d1_cols, axis = 3)\n",
        "\n",
        "        # dW\n",
        "        # Batch\n",
        "        for n in range(N):\n",
        "            for ch in range(F):\n",
        "                for row in range(OH):\n",
        "                    for col in range(OW):\n",
        "                        self.dW[ch,:,:,:] += dA[n,ch,row,col]*self.X_pad[n,:,row:row+FH, col:col+FW]\n",
        "        \n",
        "        # dB\n",
        "        # Out channel\n",
        "\n",
        "        for ch in range(F):\n",
        "            self.B[ch] = np.sum(dA[:,ch,:,:])\n",
        "        \n",
        "        # Update\n",
        "\n",
        "        self = self.optimizer.update(self)\n",
        "\n",
        "        return dZ  "
      ],
      "metadata": {
        "id": "LluZnJn1LkCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Experiment of a two-dimensional convolution layer with a small array"
      ],
      "metadata": {
        "id": "zlZSeF9RMoFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def output_shape2d(H,W,PH,PW,FH,FW,SH,SW):\n",
        "    OH = (H +2*PH -FH)/SH +1\n",
        "    OW = (W +2*PW -FW)/SW +1\n",
        "    return int(OH),int(OW)\n",
        "\n",
        "x = np.array([[[[ 1,  2,  3,  4],\n",
        "                [ 5,  6,  7,  8],\n",
        "                [ 9, 10, 11, 12],\n",
        "                [13, 14, 15, 16]]]])\n",
        "\n",
        "w = np.array([[[[ 0.,  0.,  0.],\n",
        "               [ 0.,  1.,  0.],\n",
        "               [ 0., -1.,  0.]],\n",
        "\n",
        "              [[ 0.,  0.,  0.],\n",
        "               [ 0., -1.,  1.],\n",
        "               [ 0.,  0.,  0.]]]])\n",
        "\n",
        "#w = np.array([[[[ 0.,  0.,  0.],\n",
        "#               [ 0.,  1.,  0.],\n",
        "#               [ 0., -1.,  0.]]]])\n",
        "\n",
        "#w = w[:,np.newaxis,:,:]\n",
        "N,C,H,W = x.shape\n",
        "F,C,FH,FW = w.shape\n",
        "S = 1\n",
        "P = 1\n",
        "#w = np.ones([F,C,FH,FW])\n",
        "b = np.ones((C,F))\n",
        "print(\"x shape:\", x.shape)\n",
        "print(\"w shape\", w.shape)\n",
        "#print(w)\n",
        "OH,OW = output_shape2d(H,W,P,P,FH,FW,S,S)\n",
        "X_pad = np.pad(x,((0,0),(0,0),(P,P),(P,P)))\n",
        "print(\"x pad:\", X_pad)\n",
        "#### forward ####\n",
        "A = np.zeros([N,C,OH,OW])\n",
        "for n in range(N):\n",
        "    for ch in range(C):\n",
        "        for row in range(0,H,S):\n",
        "            for col in range(0,W,S):\n",
        "                A[n,ch,row,col] = np.sum(X_pad[n,:,row:row+FH, col:col+FW] * w[:,ch,:,:]) + b[ch]\n",
        "print(\"A shape:\",A.shape)\n",
        "print(\"A:\", A)\n",
        "print(\"X_pad shape:\", X_pad.shape)\n",
        "#### Backward\n",
        "\n",
        "\n",
        "dA = np.ones(A.shape)\n",
        "dZ = np.zeros(X_pad.shape)\n",
        "dw = np.zeros(w.shape)\n",
        "db = np.zeros(b.shape)\n",
        "\n",
        "# dZ batch\n",
        "for n in range(N):\n",
        "    for ch in range(C):\n",
        "        for row in range(0,H,S):\n",
        "            for col in range(0,W,S):\n",
        "                dZ[n,:,row:row+FH, col:col+FW] += dA[n,ch,row,col]*w[:,ch,:,:]\n",
        "\n",
        "\n",
        "d1_rows = range(P), range(H+P, H+2*P,1)\n",
        "d1_cols = range(P), range(W+P, W+2*P,1)\n",
        "\n",
        "dZ = np.delete(dZ, d1_rows, axis =2)\n",
        "dZ = np.delete(dZ,d1_cols, axis = 3)\n",
        "\n",
        "# dW Batch\n",
        "for n in range(N):\n",
        "    for ch in range(C):\n",
        "        for row in range(OH):\n",
        "            for col in range(OW):\n",
        "                w[:,ch,:,:] += dA[n,ch,row,col]*X_pad[n,:,row:row+FH, col:col+FW]\n",
        "        \n",
        "# dB Out channel\n",
        "\n",
        "for ch in range(C):\n",
        "    db[ch] = np.sum(dA[:,ch,:,:])\n",
        "        \n",
        "print(\"dZ:\",dZ)\n",
        "print(\"dW:\", dw)\n",
        "print(\"db:\", db)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4O2qEq4xMonr",
        "outputId": "40830b80-87c9-49cf-a05b-4f06df580869"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x shape: (1, 1, 4, 4)\n",
            "w shape (1, 2, 3, 3)\n",
            "x pad: [[[[ 0  0  0  0  0  0]\n",
            "   [ 0  1  2  3  4  0]\n",
            "   [ 0  5  6  7  8  0]\n",
            "   [ 0  9 10 11 12  0]\n",
            "   [ 0 13 14 15 16  0]\n",
            "   [ 0  0  0  0  0  0]]]]\n",
            "A shape: (1, 2, 4, 4)\n",
            "A: [[[[ -3.  -3.  -3.  -3.]\n",
            "   [ -3.  -3.  -3.  -3.]\n",
            "   [ -3.  -3.  -3.  -3.]\n",
            "   [ 14.  15.  16.  17.]]\n",
            "\n",
            "  [[  2.   2.   2.  -3.]\n",
            "   [  2.   2.   2.  -7.]\n",
            "   [  2.   2.   2. -11.]\n",
            "   [  2.   2.   2. -15.]]]]\n",
            "X_pad shape: (1, 1, 6, 6)\n",
            "dZ: [[[[ 0.  1.  1.  1.]\n",
            "   [-1.  0.  0.  0.]\n",
            "   [-1.  0.  0.  0.]\n",
            "   [-1.  0.  0.  0.]]]]\n",
            "dW: [[[[0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]]]]\n",
            "db: [[16.]\n",
            " [16.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Problem 4] Creating a maximum pooling layer"
      ],
      "metadata": {
        "id": "skIwKnyiL6Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxPool2D():\n",
        "    '''\n",
        "    Perform max pooling\n",
        "    Parameters\n",
        "    --------------------\n",
        "    P : int \n",
        "        max pooling size\n",
        "    '''\n",
        "    def __init__(self, P):\n",
        "        self.P = P\n",
        "        self.PA = None\n",
        "        self.Pindex = None\n",
        "    \n",
        "    def forward(self, A):\n",
        "        \"\"\"\n",
        "        forward\n",
        "        Parameters\n",
        "        -------------------\n",
        "        A : ndarray shape with(n_batch, filter, height and width)\n",
        "            training samples\n",
        "        \n",
        "        \"\"\"\n",
        "        N,F,OH,OW = A.shape\n",
        "        PS = self.P\n",
        "        PH,PW = int(OH/PS), int(OW/PS)\n",
        "\n",
        "        self.params = N,F,OH,OW,PS,PH,PW\n",
        "\n",
        "        # pooling filter\n",
        "        self.PA = np.zeros([N,F,PH,PW])\n",
        "        self.Pindex = np.zeros([N,F,PH,PW])\n",
        "\n",
        "        for n in range(N):\n",
        "            for ch in range(F):\n",
        "                for row in range(PH):\n",
        "                    for col in range(PW):\n",
        "                        self.PA[n,ch,row,col] = np.max(A[n,ch,row*PS:row*PS+PS,col*PS:col*PS+PS])\n",
        "                        self.Pindex[n,ch,row,col] = np.argmax(A[n,ch,row*PS:row*PS+PS,col*PS:col*PS+PS])\n",
        "\n",
        "        return self.PA\n",
        "    \n",
        "    def backward(self, dA):\n",
        "        N,F,OH,OW,PS,PH,PW = self.params\n",
        "        dP = np.zeros([N,F,OH,OW])\n",
        "\n",
        "        for n in range(N):\n",
        "            for ch in range(F):\n",
        "                for row in range(PH):\n",
        "                    for col in range(PW):\n",
        "                        idx = self.Pindex[n,ch,row,col]\n",
        "                        tmp = np.zeros((PS*PS))\n",
        "                        for i in range(PS*PS):\n",
        "                            if i == idx:\n",
        "                                tmp[i] = dA[n,ch,row,col]\n",
        "                            else:\n",
        "                                tmp[i] = 0\n",
        "                        dP[n,ch,row*PS:row*PS+PS,col*PS:col*PS+PS] = tmp.reshape(PS,PS)\n",
        "        return dP"
      ],
      "metadata": {
        "id": "PLh7IJjeLxw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Exercise 5] (Advanced exercise) Creating an average pooling"
      ],
      "metadata": {
        "id": "jGb0PbasMF6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AvgPool2D():\n",
        "    '''\n",
        "    Perform average pooling\n",
        "    Parameters\n",
        "    --------------------\n",
        "    P : int \n",
        "         average pooling size\n",
        "    '''\n",
        "    def __init__(self, P):\n",
        "        self.P = P\n",
        "        self.PA = None\n",
        "        self.Pindex = None\n",
        "    \n",
        "    def forward(self, A):\n",
        "        \"\"\"\n",
        "        forward\n",
        "        Parameters\n",
        "        -------------------\n",
        "        A : ndarray shape with(n_batch, filter, height and width)\n",
        "            training samples\n",
        "        \n",
        "        \"\"\"\n",
        "        N,F,OH,OW = A.shape\n",
        "        PS = self.P\n",
        "        PH,PW = int(OH/PS), int(OW/PS)\n",
        "\n",
        "        self.params = N,F,OH,OW,PS,PH,PW\n",
        "\n",
        "        # pooling filter\n",
        "        self.PA = np.zeros([N,F,PH,PW])\n",
        "\n",
        "        for n in range(N):\n",
        "            for ch in range(F):\n",
        "                for row in range(PH):\n",
        "                    for col in range(PW):\n",
        "                        self.PA[n,ch,row,col] = np.mean(A[n,ch,row*PS:row*PS+PS,col*PS:col*PS+PS])\n",
        "\n",
        "        return self.PA\n",
        "    \n",
        "    def backward(self, dA):\n",
        "        N,F,OH,OW,PS,PH,PW = self.params\n",
        "        dP = np.zeros([N,F,OH,OW])\n",
        "\n",
        "        for n in range(N):\n",
        "            for ch in range(F):\n",
        "                for row in range(PH):\n",
        "                    for col in range(PW):\n",
        "                        tmp = np.zeros((PS*PS))\n",
        "                        for i in range(PS*PS):\n",
        "                                tmp[i] = dA[n,ch,row,col]/(PS*PS)\n",
        "                        dP[n,ch,row*PS:row*PS+PS,col*PS:col*PS+PS] = tmp.reshape(PS,PS)\n",
        "        return dP"
      ],
      "metadata": {
        "id": "_nPYg-IVMG10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Question 6] Smoothing"
      ],
      "metadata": {
        "id": "aSTyQhR_MMf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class flatten():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def forward(self, X):\n",
        "        self.shape = X.shape\n",
        "        return X.reshape(len(X),-1)\n",
        "    def backward(self, X):\n",
        "        return X.reshape(self.shape)"
      ],
      "metadata": {
        "id": "yQcNrCdPMNEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Question 7] Learning and Estimation"
      ],
      "metadata": {
        "id": "TY60LUAnOu6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Scratch2dCNNClassifier:\n",
        "    \"\"\"\n",
        "    2d conv layer \n",
        "    \"\"\"\n",
        "    def __init__(self, NN, CNN, n_epoch = 10, n_batch = 5, verbose = False):\n",
        "        \n",
        "        self.n_batch = n_batch\n",
        "        self.n_epoch = n_epoch\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self.log_loss = np.zeros(self.n_epoch)\n",
        "        self.log_acc = np.zeros(self.n_epoch)\n",
        "        self.NN = NN\n",
        "        self.CNN = CNN\n",
        "    def loss_function(self, y, yt):\n",
        "        delta = 1e-7\n",
        "        temp = -np.mean(yt*np.log(y + delta))\n",
        "        return temp\n",
        "    \n",
        "    def accuracy(self, y, yt):\n",
        "        return accuracy_score(y,yt)\n",
        "    \n",
        "    def fit(self, X, y, X_val = False, y_val = False):\n",
        "        \"\"\"\n",
        "        Train a cnn classifier\n",
        "\n",
        "        Parameters\n",
        "        ---------------\n",
        "        X : ndarray shape with (n_samples, n_features)\n",
        "            features of training data\n",
        "        y : ndarray shape with (n_samples, )\n",
        "            True label of training data\n",
        "        X_val : ndarray shape with (n_samples, n_features)\n",
        "            features of validation data\n",
        "        y_val : ndarray shape with (n_samples, )\n",
        "            True label of validation data\n",
        "        \"\"\"\n",
        "\n",
        "        for epoch in range(self.n_epoch):\n",
        "            self.loss = 0\n",
        "            get_mini_batch = GetMiniBatch(X,y, batch_size=self.n_batch)\n",
        "            for mini_X_train, mini_y_train in get_mini_batch:\n",
        "\n",
        "                forward_data = mini_X_train[:,np.newaxis,:,:]\n",
        "                # conv layer\n",
        "                for layer in range(len(self.CNN)):\n",
        "                    forward_data = self.CNN[layer].forward(forward_data)\n",
        "\n",
        "                #flatten layer\n",
        "                flat = flatten()\n",
        "                forward_data = flat.forward(forward_data)\n",
        "                # NN\n",
        "                for layer in range(len(self.NN)):\n",
        "                    forward_data = self.NN[layer].forward(forward_data)\n",
        "                \n",
        "                Z = forward_data\n",
        "\n",
        "                backward_data = (Z - mini_y_train)/self.n_batch\n",
        "                for layer in range(len(self.NN)-1,-1,-1):\n",
        "                    backward_data = self.NN[layer].backward(backward_data)\n",
        "\n",
        "                backward_data = flat.backward(backward_data)\n",
        "\n",
        "                for layer in range(len(self.CNN)-1,-1,-1):\n",
        "                    backward_data = self.CNN[layer].backward(backward_data)\n",
        "                \n",
        "                self.loss += self.loss_function(Z, mini_y_train)\n",
        "\n",
        "            self.log_loss[epoch] = self.loss/len(get_mini_batch)\n",
        "            self.log_acc[epoch] = self.accuracy(self.predict(X), np.argmax(y, axis = 1))\n",
        "\n",
        "            if self.verbose:\n",
        "                print('epoch:{} loss:{} acc:{}'.format(epoch, self.loss/self.n_batch, self.log_acc[epoch]))   \n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using a neural network classifier\n",
        "        \n",
        "        Parameters\n",
        "        ---------------\n",
        "        X : ndarray shape with (n_samples, n_features)\n",
        "            sample of dataset\n",
        "\n",
        "        Returns\n",
        "        ---------------\n",
        "        pred : ndarray (n_samples, 1)\n",
        "        \"\"\"\n",
        "        pred_data = X[:, np.newaxis, :,:]\n",
        "\n",
        "        for layer in range(len(self.CNN)):\n",
        "            pred_data = self.CNN[layer].forward(pred_data)\n",
        "        \n",
        "        flt=flatten()\n",
        "        pred_data = flt.forward(pred_data)\n",
        "\n",
        "        for layer in range(len(self.NN)):\n",
        "            pred_data = self.NN[layer].forward(pred_data)\n",
        "        \n",
        "        pred = np.argmax(pred_data, axis = 1)\n",
        "        return pred"
      ],
      "metadata": {
        "id": "bURWfsCcOtqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data set preparation"
      ],
      "metadata": {
        "id": "TXlWMgCPO7ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(\"X_train data shape: \", X_train.shape) # (60000, 28, 28)\n",
        "print(\"X_test data shape: \", X_test.shape) # (10000, 28, 28)\n",
        "\n",
        "# Preprocessing\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "# the correct label is an integer from 0 to 9, but it is converted to a one-hot representation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train.reshape(-1,1))\n",
        "y_val_one_hot = enc.fit_transform(y_val.reshape(-1,1))\n",
        "\n",
        "print(\"Train dataset:\", X_train.shape) # (48000, 784)\n",
        "print(\"Validation dataset:\", X_val.shape) # (12000, 784)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-wtE0dWO9N7",
        "outputId": "4212102d-e514-46d4-e9c5-b0e7060e0010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "X_train data shape:  (60000, 28, 28)\n",
            "X_test data shape:  (10000, 28, 28)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-5506699e4386>:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  X_train = X_train.astype(np.float)\n",
            "<ipython-input-9-5506699e4386>:8: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  X_test = X_test.astype(np.float)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset: (48000, 28, 28)\n",
            "Validation dataset: (12000, 28, 28)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 4 test"
      ],
      "metadata": {
        "id": "wr_iMHHHNinM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = np.random.randint(0,9,36).reshape(1,1,6,6)\n",
        "maxpooling = MaxPool2D(P=2)\n",
        "pool_forward = maxpooling.forward(test_data)\n",
        "print(\"test data:\", test_data)\n",
        "print(\"Maxpooling forward:\", pool_forward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE60RLhwNlKc",
        "outputId": "9d800a35-1db2-4fb8-8b04-1f08858b1a58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test data: [[[[8 4 1 0 4 8]\n",
            "   [3 4 3 1 5 6]\n",
            "   [4 7 7 3 8 7]\n",
            "   [8 6 4 5 2 8]\n",
            "   [1 4 7 1 5 0]\n",
            "   [1 0 7 1 8 5]]]]\n",
            "Maxpooling forward: [[[[8. 3. 8.]\n",
            "   [8. 7. 8.]\n",
            "   [4. 7. 8.]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 5 test"
      ],
      "metadata": {
        "id": "DVhPOd9wNoUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = np.random.randint(0,9,36).reshape(1,1,6,6)\n",
        "avgpooling = AvgPool2D(P=2)\n",
        "pool_forward = avgpooling.forward(test_data)\n",
        "print(\"test data:\", test_data)\n",
        "print(\"Avgpooling forward:\", pool_forward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cqdl0Z3ONo_7",
        "outputId": "7a2a930f-9520-4dd4-935c-e2a5ba708001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test data: [[[[2 4 1 0 2 5]\n",
            "   [8 0 1 2 8 5]\n",
            "   [6 4 5 5 8 3]\n",
            "   [0 7 1 2 2 3]\n",
            "   [6 4 4 3 4 6]\n",
            "   [7 0 6 3 5 7]]]]\n",
            "Avgpooling forward: [[[[3.5  1.   5.  ]\n",
            "   [4.25 3.25 4.  ]\n",
            "   [4.25 4.   5.5 ]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Problem 6 test "
      ],
      "metadata": {
        "id": "9J5F5dTVNs_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = np.zeros([10,2,5,5])\n",
        "flat = flatten()\n",
        "flat_forward = flat.forward(test_data)\n",
        "flat_backward = flat.backward(flat_forward)\n",
        "print(\"test data shape:\", test_data.shape)\n",
        "print(\"Flat forward shape:\", flat_forward.shape)\n",
        "print(\"Flat backward shape:\", flat_backward.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt4rUlAJNtjr",
        "outputId": "dc346de3-62d7-43d2-c5b7-87fbca030a5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test data shape: (10, 2, 5, 5)\n",
            "Flat forward shape: (10, 50)\n",
            "Flat backward shape: (10, 2, 5, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning and Estimation"
      ],
      "metadata": {
        "id": "Zo-ZdD28N78T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NN = {0: FC(7840, 400, HeInitializer(), AdaGrad(0.01), Tanh()),\n",
        "    1: FC(400, 200, HeInitializer(), AdaGrad(0.01), Tanh()),\n",
        "    2: FC(200, 10, SimpleInitializer(0.01), AdaGrad(0.01), Softmax()),}\n",
        "\n",
        "CNN = {0: SimpleConv2d(F=10, C=1,FH=3,FW=3,P=1,S=1,\n",
        "            initializer=SimpleInitializerConv2d(0.01), optimizer=SGD(0.01), activation=ReLU()),}\n",
        "\n",
        "cnn2d = Scratch2dCNNClassifier(NN=NN, CNN=CNN, n_epoch=10, n_batch=200, verbose = True)\n",
        "cnn2d.fit(X_train[0:1000], y_train_one_hot[0:1000])\n",
        "\n",
        "y_pred = cnn2d.predict(X_val[0:500])\n",
        "acc = accuracy_score(y_val[0:500], y_pred)\n",
        "print(\"Accuracy:\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9XW6BbJN8b7",
        "outputId": "745d4746-f2bc-46bd-9898-4b024b192364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-05b5e47c45b0>:191: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0 loss:0.005759614227384369 acc:0.091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-05b5e47c45b0>:191: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1 loss:0.005769125380470427 acc:0.091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-05b5e47c45b0>:191: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:2 loss:0.005770824112322188 acc:0.091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-05b5e47c45b0>:191: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:3 loss:0.0057710213428198275 acc:0.091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-05b5e47c45b0>:191: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:4 loss:0.0057710325785025685 acc:0.091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-05b5e47c45b0>:191: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:5 loss:0.005771037822457692 acc:0.091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-05b5e47c45b0>:191: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:6 loss:0.005771042361281595 acc:0.091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-05b5e47c45b0>:191: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:7 loss:0.005771046365240165 acc:0.091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-05b5e47c45b0>:191: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:8 loss:0.00577104962935881 acc:0.091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-05b5e47c45b0>:191: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:9 loss:0.005771052144610107 acc:0.091\n",
            "Accuracy: 0.094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(advanced task) LeNet"
      ],
      "metadata": {
        "id": "j7iFw4Bbo6uM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LeNetCNN = {0: SimpleConv2d(F=6, C=1,FH=5,FW=5,P=2,S=1,\n",
        "            initializer=SimpleInitializerConv2d(0.01), optimizer=SGD(0.1), activation=ReLU()),\n",
        "            1: MaxPool2D(P=2),\n",
        "            2: SimpleConv2d(F=16, C=6,FH=5,FW=5,P=2,S=1,\n",
        "            initializer=SimpleInitializerConv2d(0.01), optimizer=SGD(0.1), activation=ReLU()),\n",
        "            3: MaxPool2D(P=2),}\n",
        "\n",
        "LeNetNN = {0: FC(784, 120, HeInitializer(), AdaGrad(0.01), Tanh()),\n",
        "    1: FC(120, 84, HeInitializer(), AdaGrad(0.01), Tanh()),\n",
        "    2: FC(84, 10, SimpleInitializer(0.01), AdaGrad(0.01), Softmax()),}\n",
        "\n",
        "LeNet = Scratch2dCNNClassifier(NN = LeNetNN, CNN = LeNetCNN, n_epoch=10, n_batch=100, verbose=True)\n",
        "LeNet.fit(X_train[0:1000], y_train_one_hot[0:1000])\n",
        "\n",
        "y_pred_lenet = LeNet.predict(X_val[0:500])\n",
        "acc_lenet = accuracy_score(y_val[0:500], y_pred_lenet)\n",
        "print(\"Accuracy:\", acc_lenet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tx_dJ0Vyo7WK",
        "outputId": "0b486f8b-0933-4b6f-c356-9ebe4dbfe53c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-05b5e47c45b0>:191: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculation of output size and number of parameters"
      ],
      "metadata": {
        "id": "LCtkXzEEo_lQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Parameters in general are weights that are learnt during training. Parameters can calculate using following formula:\\n\\\n",
        "    (filter width*filter height*number of filter in the previous layer +1)* number of filters\")\n",
        "print(\"Example 1:\")\n",
        "print(\"input size: 144x144, 3\")\n",
        "print(\"Filter size: 3x3, 6\")\n",
        "print(\"Stride: 1\")\n",
        "print(\"Padding: None\")\n",
        "print(\"Number of parameter: 168\")\n",
        "print(\"output size: 142x142x6\")\n",
        "\n",
        "print(\"Example 2:\")\n",
        "print(\"input size: 60x60, 24\")\n",
        "print(\"Filter size: 3x3, 48\")\n",
        "print(\"Stride: 1\")\n",
        "print(\"Padding: None\")\n",
        "print(\"Number of parameter: 10416\")\n",
        "print(\"output size: 58x58x48\")\n",
        "\n",
        "print(\"Example 3:\")\n",
        "print(\"input size: 20x20, 10\")\n",
        "print(\"Filter size: 3x3, 20\")\n",
        "print(\"Stride: 2\")\n",
        "print(\"Padding: None\")\n",
        "print(\"Number of parameter: 1820\")\n",
        "print(\"output size: 9x9x20\")"
      ],
      "metadata": {
        "id": "K-pN4fNOpCHJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}